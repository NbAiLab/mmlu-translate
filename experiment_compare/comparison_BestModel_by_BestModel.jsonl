{"id": "abstract_algebra/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "abstract_algebra/dev/1", "score": 4.75, "best_models": "DeepSeek-V3"}
{"id": "abstract_algebra/dev/2", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "abstract_algebra/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "abstract_algebra/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "anatomy/dev/1", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "anatomy/dev/2", "score": 4.857142857142857, "best_models": "DeepSeek-R1"}
{"id": "anatomy/dev/3", "score": 4.714285714285714, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "anatomy/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "astronomy/dev/1", "score": 4.285714285714286, "best_models": "DeepSeek-R1"}
{"id": "astronomy/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "astronomy/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "astronomy/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "business_ethics/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "business_ethics/dev/1", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "business_ethics/dev/3", "score": 4.5, "best_models": "Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo"}
{"id": "business_ethics/dev/4", "score": 4.5, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct"}
{"id": "clinical_knowledge/dev/1", "score": 4.75, "best_models": "DeepSeek-R1"}
{"id": "clinical_knowledge/dev/2", "score": 4.5, "best_models": "DeepSeek-V3"}
{"id": "clinical_knowledge/dev/4", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "college_biology/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "college_biology/dev/3", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "college_biology/dev/4", "score": 5.0, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "college_chemistry/dev/1", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "college_chemistry/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Mistral-Small-24B-Instruct-2501"}
{"id": "college_chemistry/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct,Mistral-Small-24B-Instruct-2501"}
{"id": "college_computer_science/dev/1", "score": 4.75, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct-Turbo"}
{"id": "college_computer_science/dev/2", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "college_computer_science/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "college_mathematics/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Qwen2.5-72B-Instruct"}
{"id": "college_mathematics/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "college_mathematics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "college_medicine/dev/0", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "college_medicine/dev/2", "score": 4.75, "best_models": "DeepSeek-R1"}
{"id": "college_medicine/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "college_physics/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "college_physics/dev/1", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "college_physics/dev/2", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "college_physics/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "computer_security/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "computer_security/dev/2", "score": 4.714285714285714, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "computer_security/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "computer_security/dev/4", "score": 4.4, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "conceptual_physics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "conceptual_physics/dev/4", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "econometrics/dev/0", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "econometrics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Qwen2.5-72B-Instruct"}
{"id": "econometrics/dev/2", "score": 4.714285714285714, "best_models": "DeepSeek-V3"}
{"id": "econometrics/dev/4", "score": 4.857142857142857, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "electrical_engineering/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "electrical_engineering/dev/2", "score": 4.857142857142857, "best_models": "DeepSeek-V3"}
{"id": "electrical_engineering/dev/3", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "elementary_mathematics/dev/0", "score": 4.857142857142857, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "elementary_mathematics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "elementary_mathematics/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "elementary_mathematics/dev/3", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "elementary_mathematics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "formal_logic/dev/0", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "formal_logic/dev/1", "score": 4.833333333333333, "best_models": "DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "formal_logic/dev/2", "score": 4.666666666666667, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "formal_logic/dev/3", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "formal_logic/dev/4", "score": 4.857142857142857, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "global_facts/dev/1", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "global_facts/dev/2", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "global_facts/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "global_facts/dev/4", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_biology/dev/1", "score": 4.666666666666667, "best_models": "DeepSeek-R1"}
{"id": "high_school_biology/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,Qwen2.5-72B-Instruct"}
{"id": "high_school_chemistry/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_chemistry/dev/4", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_computer_science/dev/1", "score": 4.8, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "high_school_computer_science/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "high_school_european_history/dev/0", "score": 4.5, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_european_history/dev/1", "score": 4.142857142857143, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_european_history/dev/2", "score": 4.285714285714286, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_european_history/dev/3", "score": 4.2, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_european_history/dev/4", "score": 4.333333333333333, "best_models": "DeepSeek-V3"}
{"id": "high_school_geography/dev/0", "score": 4.714285714285714, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "high_school_geography/dev/1", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_geography/dev/2", "score": 4.5, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_geography/dev/3", "score": 4.333333333333333, "best_models": "DeepSeek-R1"}
{"id": "high_school_geography/dev/4", "score": 5.0, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "high_school_government_and_politics/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "high_school_government_and_politics/dev/1", "score": 4.857142857142857, "best_models": "DeepSeek-V3"}
{"id": "high_school_government_and_politics/dev/3", "score": 4.714285714285714, "best_models": "DeepSeek-V3"}
{"id": "high_school_government_and_politics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_macroeconomics/dev/1", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_macroeconomics/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_mathematics/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "high_school_mathematics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "high_school_mathematics/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-70B-Instruct,Mistral-Small-24B-Instruct-2501"}
{"id": "high_school_mathematics/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Qwen2.5-72B-Instruct"}
{"id": "high_school_mathematics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_microeconomics/dev/0", "score": 4.5, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_microeconomics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "high_school_microeconomics/dev/3", "score": 4.857142857142857, "best_models": "DeepSeek-V3"}
{"id": "high_school_physics/dev/0", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "high_school_physics/dev/1", "score": 5.0, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_physics/dev/2", "score": 3.5, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_physics/dev/3", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "high_school_physics/dev/4", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_psychology/dev/0", "score": 4.6, "best_models": "DeepSeek-R1"}
{"id": "high_school_psychology/dev/1", "score": 4.428571428571429, "best_models": "DeepSeek-R1"}
{"id": "high_school_psychology/dev/2", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "high_school_psychology/dev/3", "score": 4.571428571428571, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_statistics/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_statistics/dev/2", "score": 4.857142857142857, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_statistics/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "high_school_statistics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_us_history/dev/0", "score": 4.2, "best_models": "DeepSeek-R1"}
{"id": "high_school_us_history/dev/1", "score": 4.4, "best_models": "DeepSeek-V3"}
{"id": "high_school_us_history/dev/2", "score": 4.285714285714286, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_us_history/dev/3", "score": 4.333333333333333, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_us_history/dev/4", "score": 4.333333333333333, "best_models": "DeepSeek-R1"}
{"id": "high_school_world_history/dev/0", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Qwen2.5-72B-Instruct"}
{"id": "high_school_world_history/dev/2", "score": 4.666666666666667, "best_models": "DeepSeek-R1"}
{"id": "high_school_world_history/dev/3", "score": 4.2, "best_models": "DeepSeek-R1"}
{"id": "high_school_world_history/dev/4", "score": 4.666666666666667, "best_models": "DeepSeek-V3"}
{"id": "human_aging/dev/1", "score": 4.333333333333333, "best_models": "DeepSeek-R1"}
{"id": "human_aging/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "human_aging/dev/4", "score": 4.6, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "human_sexuality/dev/1", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "human_sexuality/dev/2", "score": 4.571428571428571, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct-Turbo"}
{"id": "human_sexuality/dev/3", "score": 4.25, "best_models": "DeepSeek-R1"}
{"id": "human_sexuality/dev/4", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "international_law/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "international_law/dev/2", "score": 4.571428571428571, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "international_law/dev/4", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "jurisprudence/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "jurisprudence/dev/4", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "logical_fallacies/dev/1", "score": 4.714285714285714, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "logical_fallacies/dev/3", "score": 4.666666666666667, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "logical_fallacies/dev/4", "score": 4.4, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "machine_learning/dev/1", "score": 5.0, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "machine_learning/dev/2", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "machine_learning/dev/3", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "machine_learning/dev/4", "score": 4.6, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "management/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "management/dev/1", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "management/dev/3", "score": 4.428571428571429, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "management/dev/4", "score": 5.0, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "marketing/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "marketing/dev/1", "score": 4.571428571428571, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "marketing/dev/3", "score": 4.6, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "medical_genetics/dev/1", "score": 4.833333333333333, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "medical_genetics/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "miscellaneous/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "miscellaneous/dev/1", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "miscellaneous/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct"}
{"id": "moral_disputes/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo"}
{"id": "moral_disputes/dev/2", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "moral_disputes/dev/4", "score": 4.6, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "moral_scenarios/dev/0", "score": 4.666666666666667, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "moral_scenarios/dev/1", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "moral_scenarios/dev/3", "score": 4.666666666666667, "best_models": "DeepSeek-R1"}
{"id": "nutrition/dev/0", "score": 4.857142857142857, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
{"id": "nutrition/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "nutrition/dev/4", "score": 4.857142857142857, "best_models": "DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "philosophy/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "philosophy/dev/1", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "philosophy/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "philosophy/dev/3", "score": 4.857142857142857, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "philosophy/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "prehistory/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct"}
{"id": "prehistory/dev/1", "score": 4.666666666666667, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "prehistory/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "prehistory/dev/4", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "professional_accounting/dev/0", "score": 4.571428571428571, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "professional_accounting/dev/2", "score": 4.571428571428571, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "professional_accounting/dev/3", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "professional_accounting/dev/4", "score": 4.4, "best_models": "DeepSeek-R1"}
{"id": "professional_law/dev/0", "score": 4.2, "best_models": "DeepSeek-R1"}
{"id": "professional_law/dev/1", "score": 4.285714285714286, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
{"id": "professional_law/dev/2", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "professional_law/dev/3", "score": 4.333333333333333, "best_models": "DeepSeek-R1"}
{"id": "professional_law/dev/4", "score": 4.4, "best_models": "DeepSeek-R1"}
{"id": "professional_medicine/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "professional_medicine/dev/2", "score": 4.8, "best_models": "DeepSeek-V3"}
{"id": "professional_psychology/dev/0", "score": 4.5, "best_models": "DeepSeek-V3"}
{"id": "professional_psychology/dev/1", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "professional_psychology/dev/2", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "professional_psychology/dev/4", "score": 4.857142857142857, "best_models": "DeepSeek-R1"}
{"id": "public_relations/dev/0", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "public_relations/dev/1", "score": 4.428571428571429, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "public_relations/dev/2", "score": 4.857142857142857, "best_models": "DeepSeek-R1"}
{"id": "public_relations/dev/3", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "security_studies/dev/0", "score": 4.0, "best_models": "DeepSeek-R1"}
{"id": "security_studies/dev/2", "score": 4.6, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "security_studies/dev/3", "score": 4.285714285714286, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "sociology/dev/0", "score": 4.833333333333333, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "sociology/dev/1", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "us_foreign_policy/dev/0", "score": 4.857142857142857, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct-Turbo"}
{"id": "us_foreign_policy/dev/1", "score": 4.5, "best_models": "DeepSeek-R1"}
{"id": "us_foreign_policy/dev/2", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "us_foreign_policy/dev/3", "score": 4.857142857142857, "best_models": "DeepSeek-V3"}
{"id": "us_foreign_policy/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "virology/dev/0", "score": 4.5, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "virology/dev/1", "score": 4.571428571428571, "best_models": "DeepSeek-V3"}
{"id": "virology/dev/2", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "virology/dev/4", "score": 4.857142857142857, "best_models": "DeepSeek-R1"}
{"id": "world_religions/dev/0", "score": 5.0, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "world_religions/dev/2", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "world_religions/dev/3", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "world_religions/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "anatomy/dev/0", "score": 5.0, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "astronomy/dev/0", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "business_ethics/dev/2", "score": 4.714285714285714, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "clinical_knowledge/dev/0", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "clinical_knowledge/dev/3", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "college_biology/dev/1", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo"}
{"id": "college_biology/dev/2", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "college_chemistry/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "college_chemistry/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "college_computer_science/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "college_computer_science/dev/4", "score": 4.666666666666667, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "college_mathematics/dev/1", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "college_mathematics/dev/2", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "college_medicine/dev/1", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "college_medicine/dev/3", "score": 4.833333333333333, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "college_physics/dev/4", "score": 4.8, "best_models": "DeepSeek-R1"}
{"id": "computer_security/dev/1", "score": 4.666666666666667, "best_models": "DeepSeek-R1"}
{"id": "conceptual_physics/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "conceptual_physics/dev/2", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "conceptual_physics/dev/3", "score": 4.6, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
{"id": "econometrics/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "electrical_engineering/dev/1", "score": 4.857142857142857, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-405B-Instruct"}
{"id": "electrical_engineering/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3"}
{"id": "global_facts/dev/0", "score": 5.0, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "high_school_biology/dev/0", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_biology/dev/2", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_biology/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "high_school_chemistry/dev/0", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_chemistry/dev/1", "score": 5.0, "best_models": "Mistral-Small-24B-Instruct-2501,Qwen2.5-72B-Instruct"}
{"id": "high_school_chemistry/dev/2", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "high_school_computer_science/dev/0", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_computer_science/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "high_school_computer_science/dev/3", "score": 4.714285714285714, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_government_and_politics/dev/2", "score": 4.857142857142857, "best_models": "Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_macroeconomics/dev/0", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "high_school_macroeconomics/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "high_school_macroeconomics/dev/4", "score": 4.714285714285714, "best_models": "DeepSeek-R1"}
{"id": "high_school_microeconomics/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct"}
{"id": "high_school_microeconomics/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "high_school_psychology/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct,Qwen2.5-72B-Instruct"}
{"id": "high_school_statistics/dev/0", "score": 4.833333333333333, "best_models": "DeepSeek-R1,Llama-3.3-70B-Instruct-Turbo"}
{"id": "high_school_world_history/dev/1", "score": 4.666666666666667, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "human_aging/dev/0", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "human_aging/dev/2", "score": 4.833333333333333, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "human_sexuality/dev/0", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "international_law/dev/1", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "international_law/dev/3", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "jurisprudence/dev/0", "score": 4.6, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "jurisprudence/dev/1", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "jurisprudence/dev/2", "score": 5.0, "best_models": "DeepSeek-R1"}
{"id": "logical_fallacies/dev/0", "score": 4.666666666666667, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "logical_fallacies/dev/2", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "machine_learning/dev/0", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "management/dev/2", "score": 4.857142857142857, "best_models": "Qwen2.5-72B-Instruct"}
{"id": "marketing/dev/2", "score": 4.857142857142857, "best_models": "DeepSeek-R1"}
{"id": "marketing/dev/4", "score": 4.285714285714286, "best_models": "DeepSeek-V3"}
{"id": "medical_genetics/dev/0", "score": 4.857142857142857, "best_models": "Llama-3.3-70B-Instruct-Turbo"}
{"id": "medical_genetics/dev/3", "score": 5.0, "best_models": "DeepSeek-V3"}
{"id": "medical_genetics/dev/4", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Meta-Llama-3.1-70B-Instruct"}
{"id": "miscellaneous/dev/2", "score": 5.0, "best_models": "DeepSeek-R1,DeepSeek-V3,Llama-3.3-70B-Instruct,Meta-Llama-3.1-405B-Instruct"}
{"id": "miscellaneous/dev/3", "score": 5.0, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct-Turbo,Mistral-Small-24B-Instruct-2501"}
{"id": "moral_disputes/dev/0", "score": 4.571428571428571, "best_models": "DeepSeek-R1"}
{"id": "moral_disputes/dev/3", "score": 4.714285714285714, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "moral_scenarios/dev/2", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct-Turbo,Meta-Llama-3.1-70B-Instruct"}
{"id": "moral_scenarios/dev/4", "score": 4.5, "best_models": "DeepSeek-V3,Llama-3.3-70B-Instruct"}
{"id": "nutrition/dev/2", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "nutrition/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "prehistory/dev/2", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "professional_accounting/dev/1", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-405B-Instruct"}
{"id": "professional_medicine/dev/1", "score": 4.8, "best_models": "DeepSeek-R1"}
{"id": "professional_medicine/dev/3", "score": 4.6, "best_models": "DeepSeek-R1"}
{"id": "professional_medicine/dev/4", "score": 4.857142857142857, "best_models": "Llama-3.3-70B-Instruct"}
{"id": "professional_psychology/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-R1"}
{"id": "public_relations/dev/4", "score": 5.0, "best_models": "DeepSeek-V3,Meta-Llama-3.1-405B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "security_studies/dev/1", "score": 4.666666666666667, "best_models": "Meta-Llama-3.1-70B-Instruct"}
{"id": "security_studies/dev/4", "score": 4.6, "best_models": "DeepSeek-V3"}
{"id": "sociology/dev/2", "score": 4.6, "best_models": "DeepSeek-V3"}
{"id": "sociology/dev/3", "score": 4.833333333333333, "best_models": "DeepSeek-V3"}
{"id": "sociology/dev/4", "score": 4.833333333333333, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
{"id": "virology/dev/3", "score": 5.0, "best_models": "Llama-3.3-70B-Instruct,Meta-Llama-3.1-70B-Instruct"}
{"id": "world_religions/dev/1", "score": 5.0, "best_models": "DeepSeek-R1,Meta-Llama-3.1-70B-Instruct"}
